---
title: "2008_Cheatsheet"
output:
  pdf_document: default
  html_document: default
---

                    *****LECTURE 2*****
```{r}
##### OPEN DATA #####
library("ALSM")
mydata <- TolucaCompany 
X <- mydata[,1] # predictor (lot size), or mydata$x, 1 col
Y <- mydata[,2] # response (work hours), 2nd col


##### BOX PLOT #####
boxplot(mydata) # or boxplot(X) OR boxplot (Y)


##### SCATTER PLOT #####
plot(X, Y, pch=16, xlab="Lot size", ylab = "Work hours", main="Toluca Company")
    #pch = point symbol


##### Fit model manually #####
Xbar <- mean(X)
Ybar <- mean(Y)
Xcenter <- X - Xbar # A vector
Ycenter <- Y - Ybar
Sxy <- sum(Xcenter*Ycenter) #or sum(X*Y)-length(X)*mean(X)*mean(Y)
Sxx <- sum(Xcenter^2) #or sum(X^2)-length(X)*mean(X)*mean(X)
b1 <- Sxy / Sxx # or cov(X,Y)/var(X) OR cor(X,Y)*sd(Y)/sd(X)
b0 <- Ybar - b1*Xbar

```

```{r}
##### Fit model with 'lm' function #####
mymodel <- lm(Y~X) #or lm(Y~X, data=mydata)
mymodel$coefficients #or coef(mymodel)
    # Get b0 and b1 values 
summary(mymodel) #Get b0 and b1 values and more


##### Plot fitted linear model #####
plot(X, Y, pch=16)
abline(mymodel, col='purple', lty=2, lwd=2)
    #lty = line type, lwd = line width


##### Manually find fitted values #####
mymodel <- lm(Y~X)
b0 <- coef(mymodel)$[,1] # or mymodel$coefficients[1]
b1 <- coef(mymodel)$[,2]
Yhat <- b0 + b1*X
Yfit <- mymodel$fitted.values #or fitted(mymodel)
    #Yhat and Yfit values will be identical, just format


##### Predict Y for a new observation #####
#Predict Y when new level of X = 85
### METHOD 1 ###
Xnew <- 85
Ypredict <- b0 + b1*Xnew # or mymodel$coefficients[1] + mymodel$coefficients[2]*Xnew

### METHOD 2 ###
Xnew <- c(1, 85)
Ypredict <- Xnew %*% mymodel%coefficients


##### Residuals #####
Res <- Y - Yhat # or mymodel$residuals 


##### Estimation of error terms (var) #####
SSE <- sum(Res^2)
n <- length(Y) # or dim(mydata)[1]
MSE <- SSE / (n-2) #Error (residual) mean squares
sigma_hat <- sqrt(MSE) # Estimator of stdev sigma = Residual standard error (Can be seen in summary data)
SSTO <- sum(Ycenter^2) # Has n-1 df
SSR <- sum((Yhat - Ybar)^2) # or SSTO - SSE, Has 1 df
MSR <- SSR / 1 # Regression mean squares


##### ANOVA #####
### Manually ###
Fstat <- MSR / MSE
critical_value <- qf(0.95, 1, n-2, lower.tail=TRUE) #or qf(0.05, 1, n-2, lower.tail=FALSE)
pvalue <- 1 - pf(Fstat, 1, n-2)

### Directly ###
anova(mymodel)


##### Coefficent of determination (R^2) #####
R2 <- SSR / SSTO #Proportion of total var in Y can be explained by fitted regression model
r2 <- cor(X,Y)^2 #sample correlation coefficient of X
    # Can also be found in summary data (Multiple R-squared)
```



                    *****LECTURE 3*****
```{r}
##### Sampling distribution of b1 by simulation #####
X <- 1:100
n <- 100
Sxx <- sum((X - mean(X))^2)
beta0 = 2; beta1 = 1; esigma = 1 #sigma
b0 = b1 = s_b1 = array(NA, dim=1000)

for (i in 1:1000)
{
  Y = beta0 + beta1*X + rnorm(n, 0, esigma) #mean and sigma
  mod = lm(Y~X)
  b0[i] = mod$coefficients[1]
  b1[i] = mod$coefficients[2]
  s_b1[i] = sqrt(sum(mod$residuals^2) / (n-2) / Sxx) # = sqrt(MSE)
}
mean(b1)
Sxx = crossprod(X - mean(X))
sdb1 = esigma / sqrt(Sxx)
    # True variance (Can compare using sd(b1) which is sample variance) 


##### Plotting sampling dist of b1 by simulation #####
hist(b1, prob=TRUE)
myfun = function(x) dnorm(x, beta1, sdb1)
plot(myfun, 0.985, 1.015, col='red', add=TRUE)
    # dnorm = density; 0.985 and 1.015 = the range


##### Plotting sampling dist (b1-B1) / s(b1) simulation #####
hist((b1-beta1)/s_b1, prob=TRUE)
myfun = function(x) dt(x, df=n-2)
plot(myfun, -3,3, col='red', add=TRUE)


##### Sampling distribution of b0 #####
mean(b0)
sd(b0) #sample var
sdb0 <- esigma*sqrt(1/n + mean(X)^2 / Sxx) # true var


##### CI of coefficients #####
### Manually ###
alpha <- 0.05
s_b0 <- summary(mymodel)$coefficients[1,2]
s_b1 <- summary(mymodel)$coefficients[2,2]
b0_ci <- c(b0 + qt(alpha/2, df=n-2)*s_b0,
           b0 + qt(1-alpha/2, df=n-2)*s_b0)
b1_ci <- c(b1 + qt(alpha/2, df=n-2)*s_b1,
           b1 + qt(1-alpha/2, df=n-2)*s_b1)
    # Gives lower and upper CIs for b0 and b1


### By function ###
confint(mymodel, level=0.95) #For 95% CI, both b0 and b1


##### Two-tailed test at 0 #####
summary(mymodel)$coefficents # look at t and pvalue 
    # Can 1/2 the pvalues to get 1 tailed
    # (t*)^2 = F*
    # P value smaller than 5% = Reject null


##### test at non-zero value #####
# If Beta(null) = 2
ts <- (b1-2) / s_b1 #same as critical value (?)
pvalue <- 2*(1-pt(ts, df=n-2))
  # Can 1/2 it to get 1 tailed (get rid of '2')

```

```{r}
##### CI for E(Yh) (Grp average) #####
### Manually ###
X_h <- 65
Y_hat <- b0 + b1*X_h
MSE <- sum(residuals(mymodel)^2) / df.residual(mymodel)
s_Yhat <- sqrt(MSE*(1/n+(X_h - mean(X))^2/sum((X-mean(X))^2)))
alpha <- 0.1
Y_hat_ci <- c(Y_hat - qt(1-alpha/2, n-2)*S_Yhat,
              Y_hat + qt(1-alpha/2, n-2)*S_Yhat)
names(Y_hat_ci) <- c('lower', 'upper')x

### By function ###
predict(mymodel, newdata=data.frame(X = X_h),
        interval='confidence', level=.90)


##### PI for Yhat_h_new (For individual) #####
### Manually ###
s_Yhnew <- sqrt(MSE*(1 + 1/n +
               (X_h-mean(X))^2/sum((X-mean(X))^2)))
Y_hat_pi <- c(Y_hat - qt(1-alpha/2, n-2)*s_Ynew,
              Y_hat + qt(1-alpha/2, n-2)*s_Ynew)
names(Y_hat_pi) <- c('lower', 'upper')

### By function ###
predict(mymodel, newdata=data.frame(X = X_h),
        interval='prediction', level=.90)


##### Plotting PI for Yhat_h_new #####
plot(X,Y, ylim=c(0,600))
abline(mymodel, lwd=2)
lines(X_h, ci[,2], lty=2, col=2, lwd=2)
lines(X_h, ci[,3], lty=2, col=2, lwd=2)
lines(X_h, pi[,2], lty=2, col=2, lwd=2)
lines(X_h, pi[,3], lty=2, col=2, lwd=2)
legend(20, 550, c('95CI', '95PI'), col=c(2,3), lty=c(2,2), lwd=c(2,2))


##### CI and PI for series of Xh values #####
X_h <- seq(20, 120, by=2)
ci <- predict(mymodel, newdata=data.frame(X = X_h),
        interval='confidence')
pi <- xpredict(mymodel, newdata=data.frame(X = X_h),
        interval='prediction')
```



                    *****LECTURE 4*****
```{r}
##### LINEARITY CHECK #####
plot(X, residuals(mymodel), col='orange', pch=16)
abline(h=0)
  # Gives scatter plot 


##### ERROR CONSTANT VARIANCE CHECK #####
### X and Y scatter plot ###
plot(X, Y, pch=16, col='orange')
abline(mymodel, col=4, lwd=1.5)


### fitted X and residuals scatter plot ###
plot(mymodel$fitted, mymodel$residuals, pch=16, col='orange')
abline(h=0, col=4, lwd=1.5)


##### BOX-COX TRANSFORMATION #####
boxcox(lm(Y~X, data=mydata), plotit = T)
  # Find lambda value when log-likelihood is maxxed


##### PLOTTING NORMALITY OF ERROR TERMS #####
par(mfrow=c(1,2))
qqnorm(mymodel$residuals)
qqline(mymodel$residuals)


##### INDEPENDENCE CHECK -> SEQUENCE PLOT #####
plot(1:nrow(mydata), residuals(log(mymodel)), pch=16, col='orange')


##### LEVERAGE PLOTS #####
barplot(hatvalues(mymodel))
abline(h=4/length(Y), col=2)


##### OUTLIER STUDENTIZED RESIDUALS #####
par(mfrow = c(1,2))
plot(mydata[,1], rstandard(mymodel))
  #Internal student residuals
plot (mydata[,1], rstudent(mymodel))
  # External student residuals
abline(h=4, lty=2, col=4)


#####INFLUENTIAL OBSERVATIONS -> COOKS DISTANCE #####
plot(cooks.distance(mymodel), type=h)
```



                    *****LECTURE 5*****
```{r}
##### EXPLORATORY DATA ANALYSIS #####
game_data <-  read.csv("Video_Games.csv")
sales <- game_data$Global_Sales #Y
rating <- game_data$Critic_Score #X
cor(sales, rating) 
  #Find numeric relationship between X and Y


##### DIAGNOSTICS #####
par(mfrow = c(2,2))
plot(mymodel, which = c(1,2,4))
  # Residuals + Fitted (independence + constant var), QQ plot (normality), High leverage (leverage)
plot(hatvalues(mymodel), type='h')
  # Cooks distance (influential)
abline(h=4/length(sales), lty=2)

plot(fitted(mymodel), rstudent(mymodel))
  # Find outliers (external)
  # rstandard: +/- qt(0.95, n-2)
  # rstudent: +/- qt(0.975, n-3)


##### INTERVALS (CI vs PI) #####
Xh <- seq(30, 95, 2)
ci <- predict(mymodel, newdata=data.frame(rating=Xh), interval='confidence')
pi <- predict(mymodel, newdata=data.frame(rating=Xh), interval='prediction')
plot(rating, sales, ylim=c(-6, 2.5))
abline(mymodel)
lines(Xh, ci[,2], lty=2, col=2)
lines(Xh, ci[,3], lty=2, col=2)
lines(Xh, pi[,2], lty=2, col=3)
lines(Xh, pi[,3], lty=2, col=3)
legend(30, 2, c('95CI', '95PI'), col=c(2,3), lty=c(2,2))
```

```{r}
##### CI + PI ANALYSIS #####
predict(mymodel, newdata=data.frame(rating=60), interval='confidence')
##      fit    lwr    upr
## 1   -2.7   -3.1   -2.3

predict(mymodel, newdata=data.frame(rating=60), interval='prediction')
##      fit    lwr    upr
## 1   -2.7   -5.5   0.12

# When rating is 60, estimated mean of sales or predicted sales is -2.7.

# [CONFIDENCE] When rating is 60, we are 95% confident that the mean value of sales is between -3.1 and -2.3.
  #Group

# [PREDICTION] We are 95% confident that the sales of any game with rating of 60 is between -5.5 to 0.12
  #Individual
```



                    *****LECTURE 6 *****
```{r}
##### DESIGN MATRIX #####
Y <- mydata$Y
n <- length(Y)
X <- cbind(rep(1,n), mydata$X)
  # column combination: 1st col -> Repeats of 1 n times
  #2nd col -> X values


##### COEFFICIENT ESTIMATE VIA MATRIX#####
# b = (X' X)^(-1) * X' Y
b <- solve(t(X) %*% X) %*% t(X) %*% Y
  # solve -> Get inverse of matrix
  #[1,1] -> bo while [2,1] -> b1


##### COEFFICIENT ESTIMATE BY LM #####
lm(Y~0+X)
  # Would not need '0+' if dont have design matrix
  # Design matrix -> Already have intercepts -> Set intercept to 0


##### FITTED VALUES #####
### Method 1 ###
# Yhat = Xb = X(X'X)^(-1) X'Y = HY
H <-  X %*% solve (t(X)%*%X) %*% t(X)
  # Hat matrix
Yhat <- H%*%Y
  # fitted values

### Method 2 ###
predict(lm(Y~X))


##### RESIDUALS #####
### Method 1 ###
# e = Y - Yhat = (I - H)Y
I <- diag(1,n)
e <- (I-H) %*% Y

### Method 2 ###
residuals(lm(Y~X))
```

```{r}
dw <- read.table ("Dwaine.txt")
X1 <- dw[,1]
X2 <- dw[,2]
Y <- dw[,3]

dwmod <-lm(Y~X1 + X2) # 2 predictors
dwmod2 <- lm(Y ~ poly(X1, 2) + poly(X2, 2))
  # 4 predictors (X1, X1^2), (X2, X2^2)
  # Calling the mods give intercept and relationship of predictors
```




